---
title: "Model-Based Regression from AGU 2017"
author: "Chirag Kumar and Guillermo Podesta"
output:
  html_notebook:
    toc: True
    theme: united
---

In this notebook, we explore the rationale behind the NLSST algorithm and where it fails. We investigate the
correlation between the temperature deficit and band difference. Finally, we aim to use decision trees to
estimate when a retrieval has a low or high residual magnitude based on the above exploration of when the
atmospheric correction is no longer an accurate proxy for the temperature deficit.


# Prep Workspace and Data
We use MODIS matchups from 2002 - 2016 that have been read in through a previous script. Here we simply load
the matchups, any required packages, and define necessary functions.

```{r prep_workspace, include=FALSE}
# Import necessary packages
require(ggplot2)
require(dplyr)
require(RColorBrewer)
require(circular)
require(ggmap)
require(raster)
require(rasterVis)
require(caret)
require(partykit)
require(randomForest)
require(stringr)
require(stringi)

ggplot <- function(...) {ggplot2::ggplot(...) + theme_bw()}

# Define secant function
secant.deg <- function(x) {1 / (cos(circular::rad(x)))}

# Source own functions
# Define direwctory where functions are for each operating system

if (Sys.info()["sysname"] == 'Windows') {
  fun.dir <- 'D:/matchups/r-projects/Matchup_R_Scripts/Functions/'
} else if (Sys.info()["sysname"] == 'Linux') {
  fun.dir <- '/home/ckk/Projects/Matchup_R_Scripts/Functions/'
}

fun.file <- paste0(fun.dir, 'common_functions.R')

source(file = fun.file,
  local = FALSE, echo = FALSE, verbose = FALSE)
rm(fun.dir, fun.file)


# Load data
# For 787
linux_dir <- '~/Projects/Matchup_R_Scripts/Results/objects/'
# For Laptop
#linux_dir <- '~/Projects/Matchup_R_Scripts/'
linux_file <- 'MODIS_Aqua_GSFC_ALL_Class_6.4.1_ao_2017_04_12_with_ancillary.Rdata'
AQUA_file <- paste0(linux_dir, linux_file)

if (!file.exists(AQUA_file)) {
    stop('Input file does not exist')
  } else {
    load(AQUA_file, verbose = TRUE)
    AQUA <- MODIS_Aqua_GSFC_ALL_Class_6.4.1_ao_2017_04_12
  }

rm(linux_dir, linux_file, AQUA_file)
rm(MODIS_Aqua_GSFC_ALL_Class_6.4.1_ao_2017_04_12)

set.seed(42)
filtered <- FALSE
```

The matchup file read in at a previous step is not filtered and contains too many variables. For the purpose of 
this analysis, we use only nighttime matchups with a quality of 0, 1, or 2 and only keep variables from the infrared
channels we use.

```{r prep_data, echo=TRUE}
# Turn POSIXct objects to characters bc dplyr doesn't support POSIXct
AQUA$sat.timedate <- as.character(AQUA$sat.timedate)
AQUA$buoy.timedate <- as.character(AQUA$buoy.timedate)

# Apply basic filtering
AQUA <- dplyr::tbl_df(AQUA) %>%
  dplyr::filter(solz >= 90) %>%
  dplyr::filter(qsst == 0 | qsst == 1 | qsst == 2)

# Now grab only variables that may be used to determine retrieval accuracy and make some new variables (i.e. x1, x2, x3)
# Create df of features
orig <- dplyr::tbl_df(AQUA) %>%
  dplyr::mutate(T_11 = cen.11000,
    T_12 = cen.12000,
    band.diff = T_11 - T_12,
    ref_SST = cen.ref.type.1.SST,
    x2 = band.diff * ref_SST,
    x3 = ((secant.deg(satz) - 1) * band.diff),
    lat = buoy.lat,
    lon = buoy.lon,
    sd11 = sd.11000,
    sd12 = sd.12000,
    range11 = max.11000 - min.11000,
    range12 = max.12000 - min.12000,
    diff.med.min11 = med.11000 - min.11000,
    diff.med.min12 = med.12000 - min.12000,
    qsst = qsst,
    buoy.sst = buoy.sst,
    cell5deg = cell5deg,
    buoy.timedate = buoy.timedate,
    SST.resid.SMB = cen.sst - (buoy.sst - 0.17)) %>% # SMB = sat minus buoy - also debias buoy.sst by turning buoy.sst into a skin measurement
  dplyr::select(T_11, T_12, band.diff, ref_SST, x2, x3, satz, lon, lat, sd11, sd12, range11, range12, diff.med.min11, diff.med.min12,
    qsst, buoy.sst, cell5deg, buoy.timedate, SST.resid.SMB)

```


```{r}
cutoff <- Inf

orig_out <- dplyr::tbl_df(orig) %>%
  dplyr::filter(SST.resid.SMB > cutoff)
    
orig <- dplyr::tbl_df(orig) %>%
  dplyr::filter(SST.resid.SMB <= cutoff)

orig <- as.data.frame(orig)
cutoff_filtered <- cutoff

```

```{r}
cutoff <- Inf

if (cutoff_filtered != cutoff) {
  orig_whole <- rbind(orig, orig_out)
  
  orig_out <- dplyr::tbl_df(orig) %>%
    dplyr::filter(SST.resid.SMB > cutoff)
  
  orig <- dplyr::tbl_df(orig_whole) %>%
    dplyr::filter(SST.resid.SMB <= cutoff)
  
  cutoff_filtered <- cutoff
}

```


# SST and Residuals

```{r residuals_vs_buoy_SST, echo = TRUE}

cols <- RColorBrewer::brewer.pal(n = 9, 'YlOrRd')
cols <- cols[2:9]

ggplot2::ggplot(data = orig, mapping = aes(x = buoy.sst, y = SST.resid.SMB,
  fill = cut(..count.., c(0, 50, 100, 250, 500, 1000, 2500, 5000, Inf)))) +
  geom_bin2d(bins = 75) +
  scale_fill_manual(values = cols, '# of retrievals') +
  geom_abline(slope = 0, intercept = 0, col = 'black')

```
We see that as the SST increases, so does the spread of the residuals. There are also more retrievals at higher
SSTs. We now explore the NLSST algorithm to find why there is an increase in residual spread at higher SSTs.

After the series of filters that we have applied, we are left with `r nrow(orig)` matchups, ranging from
`r min(orig$buoy.timedate)` to `r max(orig$buoy.timedate)`

# NLSST Exploration
At higher SSTs, the correlation between temperature deficit and band difference (the proxy for temperature deficit)
decreases, as the band difference begins to respond less. This can be seen in the following plot.

```{r band_diff_temp_deficit_qsst_all, echo=TRUE}

ccc <- RColorBrewer::brewer.pal(n = 8, 'YlOrRd')
ccc <- ccc[3:8] 

temp_deficit <- orig$buoy.sst - orig$T_11

# Take a sample of orig bc loess requires lots of memory - for testing purposes
index <- sample(1:nrow(orig), 50000, replace = FALSE)
zz1 <- zz1[index, ]

rrr <- ggplot2::ggplot(mapping = aes(x = orig$band.diff[index], y = temp_deficit[index])) +
  geom_bin2d(bins = 20) +
  scale_fill_gradientn(colours = ccc) +
  geom_smooth(method = lm, se = FALSE, color = 'green', alpha = 0.1) +
  geom_smooth(method = loess, se = FALSE, span = 0.4) +
  labs(x = 'T_11 - T_12', y = 'Temperature Deficit')

rrr
```


The NLSST algorithm corrects for this loss in linearity by not using just the band difference
to predict the temperature deficit but rather a product made up of the band difference times
the reference SST. While better than the trend explored in the previous plot, there is still
a distinct loss of linearity at high SSTs.

```{r product_temp_deficit, echo=TRUE}
# --- These numbers may help perform statistics for the matchups
# --- (e.g., number of matchups per cell, etc.).

temp_deficit_est <- orig$ref_SST - orig$T_11
temp_deficit <- orig$buoy.sst - orig$T_11

uuu <- data.frame(temp_deficit_est = temp_deficit_est, product = (orig$band.diff * orig$ref_SST),
                  temp_deficit = temp_deficit,
                  SST.resid.SMB = orig$SST.resid.SMB)

# Take a sample of uuu bc loess requires lots of memory - for testing purposes
set.seed(42)
index <- sample(1:nrow(uuu), 50000, replace = FALSE)
uuu <- uuu[index, ]

# --- Create a raster object with 5-degree pixels

grid5deg <- raster::raster(ncol = 20, nrow = 20,
  xmn = min(uuu$product) - 0.1, xmx = max(uuu$product) + 0.1,
  ymn = min(uuu$temp_deficit) - 0.1, ymx = max(uuu$temp_deficit) + 0.1)
  #crs = crs.string)

# --- Write cell numbers as value for the grids.
# --- Cell 1 is the upper left corner and numbers go
# --- across the top line through its end, then
# --- start again in the second line, and so forth.

raster::values(grid5deg) <- 1:raster::ncell(grid5deg) # Cell numbers for 5 deg grid

raster::cellFromRowCol(object = grid5deg, rownr = 2, colnr = 20)

raster::xyFromCell(object =  grid5deg, cell = 800, spatial = FALSE)

pts <- data.frame(product = uuu$product, temp_deficit = uuu$temp_deficit)
SST.resid.SMB <- uuu$SST.resid.SMB

count_resids <- function(residuals, ...) {
  return(length(residuals))
}

cell5deg <- raster::rasterize(pts, grid5deg, SST.resid.SMB, fun = count_resids)

ccc <- RColorBrewer::brewer.pal(n = 9, 'YlOrRd')
ccc <- ccc[3:9]
ccc <- colorRampPalette(ccc)(8)

breaks <- c(0, 50, 100, 250, 500, 1000, 2500, 5000, Inf)
#breaks <- breaks * 20

# Add loess and linear fits
Xs <- data.frame(Xs = seq(from = 0, to = 140, by = .3505))

td_lm <- lm(uuu$temp_deficit ~ uuu$product)
lm_estimates <- td_lm$coefficients[1] + (td_lm$coefficients[2] * Xs)
lm_estimates <- as.vector(lm_estimates$Xs)

td_loess <- loess(temp_deficit ~ product, data = uuu, span = 0.4)
loess_estimates <- predict(td_loess, newdata = Xs$Xs)

rasterVis::gplot(cell5deg) + 
  ggplot2::geom_raster(aes(fill = (cut(value, breaks)))) +
  scale_fill_manual(values = ccc) +
  geom_line(aes(x = Xs, y = lm_estimates), color = 'green') +
  geom_line(aes(x = Xs, y = loess_estimates), color = 'blue') +
  #scale_fill_gradientn(colours = palette) +
  labs(x = '(T_11 - T_12) * Reference SST', y = 'Temperature Deficit', fill = '# of Retrievals') +
  coord_fixed(ratio = 2.4) +
  xlim(-2, 122) +
  ylim(-.1, 22)
  #ggsave('x2_vs_TD_loess_and_linear.svg', device = 'svg', width = 8, height = 6, units = 'in')
```

At lower SSTs, there is a strong linear correlation between the product and the temperature deficit,
indicating that at low SSTs, the NLSST atmospheric correction works very well.

We also see that at higher SSTs, there is a breakdown of the linear correlation between the temperature
deficit and product used in the NLSST algorithm (x2). The temperature deficit increases more than
the band difference does, which causes increase in retrievals with negative residuals at higher
temperatures. However, the band difference alone is not enough for us to separate retrievals
with good and bad residual ranges.


```{r binning_retrievals_band_diff_temp_def_residual_a_la_raster_median, echo=TRUE}
# --- These numbers may help perform statistics for the matchups
# --- (e.g., number of matchups per cell, etc.).

temp_deficit_est <- orig$ref_SST - orig$T_11
temp_deficit <- orig$buoy.sst - orig$T_11

uuu <- data.frame(temp_deficit_est = temp_deficit_est, product = (orig$band.diff * orig$ref_SST),
                  temp_deficit = temp_deficit,
                  x3 = orig$x3,
                  sec_satz = secant.deg(orig$satz),
                  SST.resid.SMB = orig$SST.resid.SMB)

# Take a sample of uuu bc loess requires lots of memory - for testing purposes
set.seed(42)
#index <- sample(1:nrow(uuu), 50000, replace = FALSE)
#uuu <- uuu[index, ]

# Horizontalize
td_estimate_fit <- predict(td_loess, newdata = uuu$product)
uuu$td_fit_error <- uuu$temp_deficit_est - td_estimate_fit

# --- Create a raster object with 5-degree pixels

grid5deg <- raster::raster(ncol = 20, nrow = 20,
  xmn = min(uuu$product) - 0.1, xmx = max(uuu$product) + 0.1,
  ymn = min(uuu$temp_deficit) - 0.1, ymx = max(uuu$temp_deficit) + 0.1)
  #crs = crs.string)

# --- Write cell numbers as value for the grids.
# --- Cell 1 is the upper left corner and numbers go
# --- across the top line through its end, then
# --- start again in the second line, and so forth.

raster::values(grid5deg) <- 1:raster::ncell(grid5deg) # Cell numbers for 5 deg grid

raster::cellFromRowCol(object = grid5deg, rownr = 2, colnr = 20)

raster::xyFromCell(object =  grid5deg, cell = 800, spatial = FALSE)

pts <- data.frame(product = uuu$product, temp_deficit = uuu$temp_deficit)
SST.resid.SMB <- uuu$SST.resid.SMB

count_good_residuals <- function(residuals, ...) {
  #if (length(residuals) < 82) {
  #  return(-Inf)
  #}
  resid_cats <- cut(residuals, c(-Inf, -0.4, 0.4))
  N_resid_bad_low <- table(resid_cats)[1]
  N_resid_good <- table(resid_cats)[2]
  fraction_good <- N_resid_good / (N_resid_bad_low + N_resid_good)
  return(fraction_good)
}

cell5deg <- raster::rasterize(pts, grid5deg, SST.resid.SMB, fun = median) # fun can be median, IQR, or count_good_residuals

breaks_count_good <- seq(from = 0, to = 1, by = 0.1)

blues_palette <- rev(RColorBrewer::brewer.pal(9, 'Blues'))
blues_palette <- colorRampPalette(blues_palette)(13)

reds_palette <- RColorBrewer::brewer.pal(9, 'Reds')
reds_palette <- colorRampPalette(reds_palette)(5)

palette_test <- c((blues_palette), (reds_palette))

breaks_med <- c(-Inf, -6, -5.5, -5, -4.5, -4, -3.5, -3, -2.5, -2, -1.5, -1, -0.5, 0, 0.5, 1, 1.5, 2, Inf)

palette_med <- RColorBrewer::brewer.pal(n = 9, 'YlOrRd')
palette_med <- palette_med[3:9]
palette_med <- colorRampPalette(palette_med)(length(breaks_med) - 1)
palette_med <- rev(palette_med)

rasterVis::gplot(cell5deg) + 
  ggplot2::geom_raster(aes(fill = (cut(value, breaks_med)))) +
  scale_fill_manual(values = palette_test) +
  #scale_fill_gradientn(colours = palette) +
  labs(x = '(T_11 - T_12) * Reference SST', y = 'Temperature Deficit', fill = 'Median Residuals') +
  coord_fixed(ratio = 2.4) +
  xlim(-2, 122) +
  ylim(-.1, 22) +
  theme(legend.key = element_blank(), legend.key.size = unit(12, "point"))
  #ggsave('x2_vs_TD_raster_median.svg', device = 'svg', width = 8, height = 6, units = 'in')
```


```{r binning_retrievals_band_diff_temp_def_residual_a_la_raster_IQR, echo=TRUE}
# --- These numbers may help perform statistics for the matchups
# --- (e.g., number of matchups per cell, etc.).

temp_deficit_est <- orig$ref_SST - orig$T_11
temp_deficit <- orig$buoy.sst - orig$T_11

uuu <- data.frame(temp_deficit_est = temp_deficit_est, product = (orig$band.diff * orig$ref_SST),
                  temp_deficit = temp_deficit,
                  x3 = orig$x3,
                  sec_satz = secant.deg(orig$satz),
                  SST.resid.SMB = orig$SST.resid.SMB)

# Take a sample of uuu bc loess requires lots of memory - for testing purposes
set.seed(42)
#index <- sample(1:nrow(uuu), 50000, replace = FALSE)
#uuu <- uuu[index, ]

# Horizontalize
td_estimate_fit <- predict(td_loess, newdata = uuu$product)
uuu$td_fit_error <- uuu$temp_deficit_est - td_estimate_fit

# --- Create a raster object with 5-degree pixels

grid5deg <- raster::raster(ncol = 20, nrow = 20,
  xmn = min(uuu$product) - 0.1, xmx = max(uuu$product) + 0.1,
  ymn = min(uuu$temp_deficit) - 0.1, ymx = max(uuu$temp_deficit) + 0.1)
  #crs = crs.string)

# --- Write cell numbers as value for the grids.
# --- Cell 1 is the upper left corner and numbers go
# --- across the top line through its end, then
# --- start again in the second line, and so forth.

raster::values(grid5deg) <- 1:raster::ncell(grid5deg) # Cell numbers for 5 deg grid

raster::cellFromRowCol(object = grid5deg, rownr = 2, colnr = 20)

raster::xyFromCell(object =  grid5deg, cell = 800, spatial = FALSE)

pts <- data.frame(product = uuu$product, temp_deficit = uuu$temp_deficit)
SST.resid.SMB <- uuu$SST.resid.SMB

cell5deg <- raster::rasterize(pts, grid5deg, SST.resid.SMB, fun = IQR) # fun can be median, IQR, or count_good_residuals

breaks_IQR <- c(seq(from = 0, to = 1.5, by = 0.1), Inf)

palette_IQR <- RColorBrewer::brewer.pal(9, 'Reds')
#palette_IQR <- palette_IQR[5:9]
palette_IQR <- colorRampPalette(palette_IQR)(length(breaks_IQR) - 1)

rasterVis::gplot(cell5deg) + 
  ggplot2::geom_raster(aes(fill = (cut(value, breaks_IQR)))) +
  scale_fill_manual(values = palette_IQR) +
  #scale_fill_gradientn(colours = palette) +
  labs(x = '(T_11 - T_12) * Reference SST', y = 'Temperature Deficit', fill = 'IQR of Residuals') +
  coord_fixed(ratio = 2.4) +
  xlim(-2, 122) +
  ylim(-.1, 22) +
  theme(legend.key = element_blank(), legend.key.size = unit(12, "point"))
  #ggsave('x2_vs_TD_raster_IQR.svg', device = 'svg', width = 8, height = 6, units = 'in')
```


[TEXT TO INSERT]

# Decision Trees

## Prep Data for Decision Trees

### Create Additional Variables

Explored before is that lines can be drawn that are a function of the product and temperature deficit
that segment good and bad retrievals. However, we can't give the temperature deficit (the answer) to
the machine learning model. We create estimates of the temperature deficit using the reference SST
minus T_11 (T_12).

```{r create_additional_variables, echo=TRUE}
# Month
orig$month <- lubridate::month(as.POSIXct(orig$buoy.timedate))

# Secant of the satz
orig$sec_satz <- secant.deg(orig$satz)

# Temperature Deficit Estimate
orig$ref_SST_minus_T_11 <- orig$ref_SST - orig$T_11

# Linearized temperature deficit estimate as a function of NLSST product
orig$td_resid <- orig$ref_SST_minus_T_11 - predict(td_loess, newdata = orig$x2)

# Loess can only interpolate
# Those values of x2 that are outside the domain get put in as NA
# Remove those
orig <- dplyr::tbl_df(orig) %>%
  dplyr::filter(!is.na(td_resid))


orig <- as.data.frame(orig)

```

### Split into Train and Test Sets

All our algorithms will be trained on 60% of our data and tested on the other 40%.
We make that split here.

```{r create_train_test_set, echo=TRUE}
# Divide data into training and validation sets
prop_train <- .6
len_train <- floor(prop_train * nrow(orig))
index <- sample(1:nrow(orig), len_train, replace = FALSE)

orig_train <- orig[index, ]
orig_test <- orig[-index, ]
```


### Regression

```{r ctree_big_4_variable_tree_regr, echo=TRUE}

# Build the tree
ctree_big_4_var_tree_regr <- partykit::ctree(formula = SST.resid.SMB ~ td_resid + x2 + sec_satz + sd11 + lat + month,
                                 data = orig_train,
                                 minbucket = 82,
                                 maxdepth = 7,
                                 mincrit = 0.95)
# Plot the tree
#plot(partykit::as.party(rpart_big_4_var_tree_regr))

# Evaluate the tree
## Use the tree to predict
### Train set
ctree_big_4_var_tree_train_vals <- stats::predict(object = ctree_big_4_var_tree_regr, newdata = orig_train, na.action = na.pass)

### Test set
ctree_big_4_var_tree_test_vals <- stats::predict(object = ctree_big_4_var_tree_regr, newdata = orig_test, na.action = na.pass)

### Train set
rmse_train_ctree <- caret::RMSE(ctree_big_4_var_tree_train_vals, orig_train$SST.resid.SMB)
rmse_train_ctree
cor(ctree_big_4_var_tree_train_vals, orig_train$SST.resid.SMB)

### Test set
rmse_test_ctree <- caret::RMSE(ctree_big_4_var_tree_test_vals, orig_test$SST.resid.SMB)
rmse_test_ctree
cor(ctree_big_4_var_tree_test_vals, orig_test$SST.resid.SMB)

ggplot2::ggplot() +
  geom_point(aes(x = ((ctree_big_4_var_tree_test_vals)), orig_test$SST.resid.SMB), pch = '.')

fit_ctree_lm <- lm(orig_test$SST.resid.SMB ~ ctree_big_4_var_tree_test_vals)
```

```{r evaluate_ctree_regression_statistically, echo=TRUE}

window <- .1

dt_model_residuals_train <- ctree_big_4_var_tree_train_vals - orig_train$SST.resid.SMB

table(abs(dt_model_residuals_train) < window)
prop.table(table(abs(dt_model_residuals_train) < window))
IQR(dt_model_residuals_train)


dt_model_residuals_test <- ctree_big_4_var_tree_test_vals - orig_test$SST.resid.SMB

table(abs(dt_model_residuals_test) < window)
prop.table(table(abs(dt_model_residuals_test) < window))
IQR(dt_model_residuals_test)


```


```{r explore_regression_ctree}
ctree_big_4_var_tree_train_nodes <- stats::predict(object = ctree_big_4_var_tree_regr, newdata = orig_train, type = 'node', na.action = na.pass)

medians_ctree <- tapply(orig_train$SST.resid.SMB, ctree_big_4_var_tree_train_nodes, median)
IQRs_ctree <- tapply(orig_train$SST.resid.SMB, ctree_big_4_var_tree_train_nodes, IQR)
lengths_ctree <- tapply(orig_train$SST.resid.SMB, ctree_big_4_var_tree_train_nodes, length)

node_predictions_ctree <- data.frame(SST.resid.SMB = orig_train$SST.resid.SMB, node = ctree_big_4_var_tree_train_nodes)

ggplot2::ggplot(data = node_predictions_ctree) +
  geom_histogram(aes(SST.resid.SMB)) +
  facet_wrap(~node)

```


## Cubist

```{r Cubist, echo=TRUE}

uuu <- orig_train
set.seed(108)
index <- sample(1:nrow(uuu), 50000, replace = FALSE)
uuu <- uuu[index, ]

Cubist_rules <- Cubist::cubist(x = orig_train[, c('lat', 'month', 'sd11', 'td_resid', 'x2', 'sec_satz')], y = orig_train$SST.resid.SMB,
                           committees = 5,
                           # Full data set: __ rules (no lat or month), 55 rules (both lat and month) 
                           control = Cubist::cubistControl(rules = 44,
                                                           unbiased = FALSE,
                                                           extrapolation = 100,
                                                           label = 'SST Residual',
                                                           seed = 108))

capture.output(summary(Cubist_rules), file = 'Cubist_rules=44_committees=5_with_lat_and_month.txt')

dotplot(Cubist_rules, what = 'splits')
dotplot(Cubist_rules, what = 'coefs')

# Evaluate the tree
## Use the tree to predict
### Train set
N_correction_neighbors <- 9 # Must be between 0 and 9

Cubist_rules_train_vals <- stats::predict(object = Cubist_rules, newdata = orig_train, na.action = na.pass, neighbors = N_correction_neighbors)

### Test set
Cubist_rules_test_vals <- stats::predict(object = Cubist_rules, newdata = orig_test, na.action = na.pass, neighbors = N_correction_neighbors)


### Train set
RMSE_train_cubist <- caret::RMSE(Cubist_rules_train_vals, orig_train$SST.resid.SMB)
RMSE_train_cubist
MAE_train_cubist <- mean(abs(Cubist_rules_train_vals - orig_train$SST.resid.SMB))
MAE_train_cubist
cor(Cubist_rules_train_vals, orig_train$SST.resid.SMB)

### Test set
RMSE_test_cubist <- caret::RMSE(Cubist_rules_test_vals, orig_test$SST.resid.SMB)
RMSE_test_cubist
MAE_test_cubist <- mean(abs(Cubist_rules_test_vals - orig_test$SST.resid.SMB))
MAE_test_cubist
cor(Cubist_rules_test_vals, orig_test$SST.resid.SMB)

ggplot2::ggplot() +
  geom_point(aes(x = Cubist_rules_test_vals, orig_test$SST.resid.SMB), pch = '.') +
  geom_abline(slope = 1, intercept = 0, color = 'tomato') +
  labs(x = 'Predicted Residual from the Cubist Decision Tree Regression Algorithm', y = 'Actual Residual')
  #ggsave('M5P_predictions_vs_actual_residual.svg', device = 'svg', width = 8, height = 6, units = 'in')



ccc <- RColorBrewer::brewer.pal(n = 9, 'YlOrRd')
ccc <- ccc[3:9]
breaks <- c(0, 10, 25, 50, 100, 500, 1000, 2500, 5000, 10000, Inf)

yyy <- data.frame(Y = Cubist_rules_test_vals, X = orig_test$SST.resid.SMB)

ccc <- colorRampPalette(ccc)(length(breaks) - 1)

ggplot2::ggplot(data = yyy, aes(X, Y, fill = cut(..count.., c(0, 10, 25, 50, 100, 500, 1000, 2500, 5000, 10000, Inf)))) +
  geom_bin2d(bins = 75) +
  scale_fill_manual(values = ccc, '# of Retrievals') +
  geom_abline(slope = 1, intercept = 0, color = 'black') +
  labs(x = 'Actual Residual', y = 'Predicted Residual from the Cubist Decision Tree Regression Algorithm')
  #ggsave('Cubist_predictions_vs_actual_residual_2d_histogram.pdf', device = 'pdf', width = 8, height = 6, units = 'in')

residual_fit_cubist <- lm(Y ~ X, data = yyy)
residual_fit_cubist

```

```{r grid_search_for_committees_and_rules, echo = TRUE}
set.seed(108)
too_small <- FALSE
committees <- seq(from = 1, to = 25)
rules <- seq(from = 1, to = 100)
ctrl_params_performance <- expand.grid(rules = rules, committees = committees)
ctrl_params_performance$RMSE_train <- rep(NA, nrow(ctrl_params_performance))
ctrl_params_performance$RMSE_test <- rep(NA, nrow(ctrl_params_performance))

for (N in 1:nrow(ctrl_params_performance)) {
  committees_specific <- ctrl_params_performance[N, 'committees']
  rules_specific <- ctrl_params_performance[N, 'rules']
  
  if (rules_specific == 1) {
    too_small <- FALSE
  }
  
  if (!too_small) {
   
    Cubist_rules <- Cubist::cubist(x = orig_train[, c('lat', 'month', 'sd11', 'td_resid', 'x2', 'sec_satz')], y = orig_train$SST.resid.SMB,
                           committees = committees_specific, 
                           control = Cubist::cubistControl(rules = rules_specific,
                                                           unbiased = FALSE,
                                                           extrapolation = 100,
                                                           label = 'SST Residual',
                                                           seed = 108))
  
    file <- paste0('Cubist_Rule_Sets/Cubist_rules=', rules_specific, '_committees=', committees_specific, '_with_lat_and_month.txt')
    capture.output(summary(Cubist_rules), file = file)
  
    rule_summary <- summary(Cubist_rules)
  
    node_sizes <- rep(NA, rules_specific)
    node_sizes <- as.data.frame(node_sizes)
    mmm <- strsplit(paste(rule_summary), '\n')
    mmm <- as.vector(unlist(mmm))
    counter <- 0
    for (K in 1:length(mmm)) {
      line_specific <- mmm[K]
    
      if (substring(line_specific, 1, 7) == "  Rule ") {
        cases_in_node <- stringr::str_extract(line_specific, '[0-9]+ cases')
        cases_in_node <- substring(cases_in_node, 1, stringi::stri_length(cases_in_node) - 6)
        cases_in_node <- as.numeric(cases_in_node)
      
        node_sizes[counter + 1] <- cases_in_node
        counter <- counter + 1
      }
    
    }
  
    if (min(node_sizes) >= 100) {
      # Compute the RMSE on a random sample of data 
    
      N_correction_neighbors <- 9 # Must be between 0 and 9
    
      uuu <- orig_train
      index <- sample(1:nrow(uuu), 50000, replace = FALSE)
      uuu <- uuu[index, ]
    
      zzz <- orig_test
      index <- sample(1:nrow(zzz), 50000, replace = FALSE)
      zzz <- zzz[index, ]
    
      ### Train set
      Cubist_rules_train_vals <- stats::predict(object = Cubist_rules, newdata = uuu, na.action = na.pass, neighbors = N_correction_neighbors)

      ### Test set
      Cubist_rules_test_vals <- stats::predict(object = Cubist_rules, newdata = zzz, na.action = na.pass, neighbors = N_correction_neighbors)

      ### Train set
      RMSE_train_cubist <- caret::RMSE(Cubist_rules_train_vals, uuu$SST.resid.SMB)
      ctrl_params_performance[N, 'RMSE_train'] <- RMSE_train_cubist

      ### Test set
      RMSE_test_cubist <- caret::RMSE(Cubist_rules_test_vals, zzz$SST.resid.SMB)
      ctrl_params_performance[N, 'RMSE_test'] <- RMSE_test_cubist
    
      too_small <- FALSE
    
      cat('Committees:', committees_specific, 'out of 25; Rules:', rules_specific, 'out of 100; RMSE Test:', RMSE_test_cubist, '\n')
      
    } else{
    
      too_small <- TRUE
    } 
    
  }
  
}

```



```{r expand_grid_to_find_best_neighbors, echo = TRUE}
cb_grid <- expand.grid(committees = c(1), neighbors = c(1:9))

set.seed(42)

cb_tune_train <- caret::train(x = orig_train[, c('lat', 'month', 'sd11', 'td_resid', 'x2', 'sec_satz')], y = orig_train$SST.resid.SMB,
                              method = 'cubist',
                              tuneGrid = cb_grid,
                              trcontrol = trainControl(method = 'repeatedcv', repeats = 5))

ggplot(cb_tune_train)


```




```{r evaluate_Cubist_regression_statistically, echo=TRUE}

window <- .2

Cubist_rules_residuals_train <- Cubist_rules_train_vals - orig_train$SST.resid.SMB

table(abs(Cubist_rules_residuals_train) < window)
prop.table(table(abs(Cubist_rules_residuals_train) < window))
IQR(M5P_model_residuals_train)


Cubist_rules_residuals_test <- Cubist_rules_test_vals - orig_test$SST.resid.SMB

table(abs(Cubist_rules_residuals_test) < window)
prop.table(table(abs(Cubist_rules_residuals_test) < window))
IQR(Cubist_rules_residuals_test)

```

```{r evaluate_Cubist_regression_statistically_plot, echo=TRUE}
windows <- seq(from = 0, to = 1.0, by = .05)

prob_in_window <- windows

for (i in seq(1, length(windows))) {
  Cubist_rules_residuals_test <- Cubist_rules_test_vals - orig_test$SST.resid.SMB

  window <- windows[i]
  ooo <- prop.table(table(abs(Cubist_rules_residuals_test) < window))
  prob_in_window[i] <- ooo[2]
}

prob_in_window <- prob_in_window * 100

ggplot2::ggplot() +
  geom_point(aes(x = windows, y = prob_in_window)) +
  xlim(0, 1.0) +
  ylim(0, 100) +
  labs(x = 'Plus/Minus Interval', y = '% Retrievals with abs(Predicted - Actual Residual) < Interval') +
  ggsave('M5P_predictions_within_interval.svg', device = 'svg', width = 8, height = 6, units = 'in')

```


```{r rule_distribution_Cubist, echo=TRUE}

# Rules for tree without latitude and seasonality
# Rule 1:
rule1 <- dplyr::tbl_df(orig_train) %>%
  dplyr::filter(td_resid > 1.013252) %>%
  dplyr::filter(x2 <= 21.7106) %>%
  dplyr::filter(sec_satz <= 1.536021) %>%
  dplyr::filter(sd11 <= 0.1656)

ggplot2::ggplot() +
  geom_histogram(data = rule1, aes(SST.resid.SMB), bins = 20) +
  xlim(-7, 0.5) +
  labs(x = 'SST Residual', y = 'N of Retrievals') +
  ggsave('Cubist_residual_distribution_rule_1.pdf', device = 'pdf', width = 8, height = 6, units = 'in')



rule2 <- dplyr::tbl_df(orig_train) %>%
  dplyr::filter(td_resid > 0.4383563) %>%
  dplyr::filter(x2 > 21.7106) %>%
  dplyr::filter(x2 <= 53.802) %>%
  dplyr::filter(sec_satz <= 1.536021) %>%
  dplyr::filter(sd11 > 0.1108)

ggplot2::ggplot() +
  geom_histogram(data = rule2, aes(SST.resid.SMB), bins = 20) +
  xlim(-7, 0.5) +
  labs(x = 'SST Residual', y = 'N of Retrievals') +
  ggsave('Cubist_residual_distribution_rule_2.pdf', device = 'pdf', width = 8, height = 6, units = 'in')

```


