---
title: "NLSST Exploration for AGU"
author: "Chirag Kumar and Guillermo Podesta"
output:
  html_notebook:
    toc: True
    theme: united
---

In this notebook, we explore the rationale behind the NLSST algorithm and where it fails. We investigate the
correlation between the temperature deficit and band difference. Finally, we aim to use decision trees to
estimate when a retrieval has a low or high residual magnitude based on the above exploration of when the
atmospheric correction is no longer an accurate proxy for the temperature deficit.


# Prep Workspace and Data
We use MODIS matchups from 2002 - 2016 that have been read in through a previous script. Here we simply load
the matchups, any required packages, and define necessary functions.

```{r prep_workspace, include=FALSE}
# Import necessary packages
require(ggplot2)
require(dplyr)
require(RColorBrewer)
require(circular)
require(ggmap)
require(raster)
require(sfsmisc)
require(rasterVis)
require(rgdal)
require(rpart)
require(rpart.plot)
require(caret)
require(evtree)
require(partykit)
require(randomForest)
require(RWeka)
require(DMwR)
require(wesanderson)
require(C50)
require(gbm)
require(MASS)
require(reshape2)
require(directlabels)

ggplot <- function(...) {ggplot2::ggplot(...) + theme_bw()}

# Define secant function
secant.deg <- function(x) {1 / (cos(circular::rad(x)))}

# Source own functions
# Define direwctory where functions are for each operating system

if (Sys.info()["sysname"] == 'Windows') {
  fun.dir <- 'D:/matchups/r-projects/Matchup_R_Scripts/Functions/'
} else if (Sys.info()["sysname"] == 'Linux') {
  fun.dir <- '/home/ckk/Projects/Matchup_R_Scripts/Functions/'
}

fun.file <- paste0(fun.dir, 'common_functions.R')

source(file = fun.file,
  local = FALSE, echo = FALSE, verbose = FALSE)
rm(fun.dir, fun.file)


# Load data
# For 787
linux_dir <- '~/Projects/Matchup_R_Scripts/Results/objects/'
# For Laptop
#linux_dir <- '~/Projects/Matchup_R_Scripts/'
linux_file <- 'MODIS_Aqua_GSFC_ALL_Class_6.4.1_ao_2017_04_12_with_ancillary.Rdata'
AQUA_file <- paste0(linux_dir, linux_file)

if (!file.exists(AQUA_file)) {
    stop('Input file does not exist')
  } else {
    load(AQUA_file, verbose = TRUE)
    AQUA <- MODIS_Aqua_GSFC_ALL_Class_6.4.1_ao_2017_04_12
  }

rm(linux_dir, linux_file, AQUA_file)
rm(MODIS_Aqua_GSFC_ALL_Class_6.4.1_ao_2017_04_12)

set.seed(42)
```

The matchup file read in at a previous step is not filtered and contains too many variables. For the purpose of 
this analysis, we use only nighttime matchups with a quality of 0, 1, or 2 and only keep variables from the infrared
channels we use.

```{r prep_data, echo=TRUE}
# Turn POSIXct objects to characters bc dplyr doesn't support POSIXct
AQUA$sat.timedate <- as.character(AQUA$sat.timedate)
AQUA$buoy.timedate <- as.character(AQUA$buoy.timedate)

# Apply basic filtering
AQUA <- dplyr::tbl_df(AQUA) %>%
  dplyr::filter(solz >= 90) %>%
  dplyr::filter(qsst == 0 | qsst == 1 | qsst == 2)

# Now grab only variables that may be used to determine retrieval accuracy and make some new variables (i.e. x1, x2, x3)
# Create df of features
orig <- dplyr::tbl_df(AQUA) %>%
  dplyr::mutate(T_11 = cen.11000,
    T_12 = cen.12000,
    band.diff = T_11 - T_12,
    ref_SST = cen.ref.type.1.SST,
    x2 = band.diff * ref_SST,
    x3 = ((secant.deg(satz) - 1) * band.diff),
    lat = buoy.lat,
    lon = buoy.lon,
    sd11 = sd.11000,
    sd12 = sd.12000,
    range11 = max.11000 - min.11000,
    range12 = max.12000 - min.12000,
    diff.med.min11 = med.11000 - min.11000,
    diff.med.min12 = med.12000 - min.12000,
    qsst = qsst,
    buoy.sst = buoy.sst,
    cell5deg = cell5deg,
    buoy.timedate = buoy.timedate,
    SST.resid.SMB = cen.sst - (buoy.sst - 0.17)) %>% # SMB = sat minus buoy - also debias buoy.sst by turning buoy.sst into a skin measurement
  dplyr::select(T_11, T_12, band.diff, ref_SST, x2, x3, satz, lon, lat, sd11, sd12, range11, range12, diff.med.min11, diff.med.min12,
    qsst, buoy.sst, cell5deg, buoy.timedate, SST.resid.SMB)

```

# SST and Residuals

```{r residuals_vs_buoy_SST, echo = TRUE}

cols <- RColorBrewer::brewer.pal(n = 9, 'YlOrRd')
cols <- cols[2:9]

ggplot2::ggplot(data = orig, mapping = aes(x = buoy.sst, y = SST.resid.SMB,
  fill = cut(..count.., c(0, 50, 100, 250, 500, 1000, 2500, 5000, Inf)))) +
  geom_bin2d(bins = 75) +
  scale_fill_manual(values = cols, '# of retrievals') +
  geom_abline(slope = 0, intercept = 0, col = 'black')

```
We see that as the SST increases, so does the spread of the residuals. There are also more retrievals at higher
SSTs. We now explore the NLSST algorithm to find why there is an increase in residual spread at higher SSTs.

## Residual Distribution

```{r resid_distribution, echo=TRUE}
resid_cats <- cut(orig$SST.resid.SMB, c(-Inf, -0.4, 0.4, Inf))
table(resid_cats)
```

Because of the very few retrievals that have residuals > 0.4, we choose to discard these from our analysis.
Furthermore, these residuals are only causes by extremely abnormal conditions and are hence easier to
identify. For ease, we refer to retrievals less than -0.4 as bad and retrievals inbetween -0.4 and 0.4 as
good.

```{r resid_filtering, echo=TRUE}
orig <- dplyr::tbl_df(orig) %>%
  dplyr::filter(SST.resid.SMB <= 0.4)

orig <- as.data.frame(orig)
summary(orig$SST.resid.SMB)

resid_cats <- cut(orig$SST.resid.SMB, c(-Inf, -0.4, 0.4, Inf))
table(resid_cats)
```

After the series of filters that we have applied, we are left with `r nrow(orig)` matchups, ranging from
`r min(orig$buoy.timedate)` to `r max(orig$buoy.timedate)`

# NLSST Exploration
At higher SSTs, the correlation between temperature deficit and band difference (the proxy for temperature deficit)
decreases, as the band difference begins to respond less. This can be seen in the following plot.

```{r band_diff_temp_deficit_qsst_all, echo=TRUE}

ccc <- RColorBrewer::brewer.pal(n = 8, 'YlOrRd')
ccc <- ccc[3:8] 

temp_deficit <- orig$buoy.sst - orig$T_11

# Take a sample of orig bc loess requires lots of memory - for testing purposes
index <- sample(1:nrow(orig), 50000, replace = FALSE)
zz1 <- zz1[index, ]

rrr <- ggplot2::ggplot(mapping = aes(x = orig$band.diff[index], y = temp_deficit[index])) +
  geom_bin2d(bins = 20) +
  scale_fill_gradientn(colours = ccc) +
  geom_smooth(method = lm, se = FALSE, color = 'green', alpha = 0.1) +
  geom_smooth(method = loess, se = FALSE, span = 0.4) +
  labs(x = 'T_11 - T_12', y = 'Temperature Deficit')

rrr
```


The NLSST algorithm corrects for this loss in linearity by not using just the band difference
to predict the temperature deficit but rather a product made up of the band difference times
the reference SST. While better than the trend explored in the previous plot, there is still
a distinct loss of linearity at high SSTs.

```{r product_temp_deficit, echo=TRUE}
# --- These numbers may help perform statistics for the matchups
# --- (e.g., number of matchups per cell, etc.).

temp_deficit_est <- orig$ref_SST - orig$T_11
temp_deficit <- orig$buoy.sst - orig$T_11

uuu <- data.frame(resid_context = as.factor(resid_context), temp_deficit_est = temp_deficit_est, product = (orig$band.diff * orig$ref_SST),
                  temp_deficit = temp_deficit,
                  SST.resid.SMB = orig$SST.resid.SMB)

# Take a sample of uuu bc loess requires lots of memory - for testing purposes
set.seed(42)
#index <- sample(1:nrow(uuu), 50000, replace = FALSE)
#uuu <- uuu[index, ]

# --- Create a raster object with 5-degree pixels

grid5deg <- raster::raster(ncol = 20, nrow = 20,
  xmn = min(uuu$product) - 0.1, xmx = max(uuu$product) + 0.1,
  ymn = min(uuu$temp_deficit) - 0.1, ymx = max(uuu$temp_deficit) + 0.1)
  #crs = crs.string)

# --- Write cell numbers as value for the grids.
# --- Cell 1 is the upper left corner and numbers go
# --- across the top line through its end, then
# --- start again in the second line, and so forth.

raster::values(grid5deg) <- 1:raster::ncell(grid5deg) # Cell numbers for 5 deg grid

raster::cellFromRowCol(object = grid5deg, rownr = 2, colnr = 20)

raster::xyFromCell(object =  grid5deg, cell = 800, spatial = FALSE)

pts <- data.frame(product = uuu$product, temp_deficit = uuu$temp_deficit)
SST.resid.SMB <- uuu$SST.resid.SMB

count_resids <- function(residuals, ...) {
  return(length(residuals))
}

cell5deg <- raster::rasterize(pts, grid5deg, SST.resid.SMB, fun = count_resids)

ccc <- RColorBrewer::brewer.pal(n = 9, 'YlOrRd')
ccc <- ccc[3:9]

breaks <- c(0, 50, 100, 250, 500, 1000, 2500, 5000, Inf)
breaks <- breaks * 20

# Add loess and linear fits
Xs <- data.frame(Xs = seq(from = 0, to = 140, by = .3505))

td_lm <- lm(uuu$temp_deficit ~ uuu$product)
lm_estimates <- td_lm$coefficients[1] + (td_lm$coefficients[2] * Xs)
lm_estimates <- as.vector(lm_estimates$Xs)

td_loess <- loess(temp_deficit ~ product, data = uuu, span = 0.4)
loess_estimates <- predict(td_loess, newdata = Xs$Xs)

rasterVis::gplot(cell5deg) + 
  ggplot2::geom_raster(aes(fill = (cut(value, breaks)))) +
  scale_fill_manual(values = ccc) +
  geom_line(aes(x = Xs, y = lm_estimates), color = 'green') +
  geom_line(aes(x = Xs, y = loess_estimates), color = 'blue') +
  #scale_fill_gradientn(colours = palette) +
  labs(x = '(T_11 - T_12) * Reference SST', y = 'Temperature Deficit', fill = '# of Retrievals') +
  coord_fixed(ratio = 2.4) +
  xlim(-2, 122) +
  ylim(-.1, 22) + 
  ggsave('x2_vs_TD_loess_and_linear.svg', device = 'svg', width = 8, height = 6, units = 'in')
```

At lower SSTs, there is a strong linear correlation between the product and the temperature deficit,
indicating that at low SSTs, the NLSST atmospheric correction works very well.

We also see that at higher SSTs, there is a breakdown of the linear correlation between the temperature
deficit and product used in the NLSST algorithm (x2). The temperature deficit increases more than
the band difference does, which causes increase in retrievals with negative residuals at higher
temperatures. However, the band difference alone is not enough for us to separate retrievals
with good and bad residual ranges.

```{r band_diff_temp_deficit_contours, echo=TRUE}

resid_context <- cut(orig$SST.resid.SMB, c(-Inf, -.4, .4, Inf))
temp_deficit <- orig$buoy.sst - orig$T_11

uuu <- data.frame(resid_context = as.factor(resid_context), temp_deficit = temp_deficit, product = (orig$band.diff * orig$ref_SST))

# Take a sample of uuu bc loess requires lots of memory - for testing purposes
set.seed(42)
index <- sample(1:nrow(uuu), 50000, replace = FALSE)
uuu <- uuu[index, ]


ggplot2::ggplot(data = uuu, mapping = aes(x = product, y = temp_deficit, color = uuu$resid_context)) +
  geom_density_2d() +
  labs(x = '(T_11 - T_12) * Reference SST', y = 'Temperature Deficit', color = 'Residual Range')
  #ggsave('band_diff_predictor_of_temp_deficit_different_in_situ_windows.pdf')

```



```{r band_diff_temp_deficit_cumulative_contours, echo=TRUE}
require(ggplot2)
require(MASS)
require(reshape2)
require(directlabels)

# data:
resid_context <- cut(orig$SST.resid.SMB, c(-Inf, -.4, .4, Inf))
temp_deficit <- orig$buoy.sst - orig$T_11

uuu <- data.frame(resid_context = as.factor(resid_context), temp_deficit = temp_deficit, product = (orig$band.diff * orig$ref_SST))

# Take a sample of uuu bc loess requires lots of memory - for testing purposes
set.seed(42)
#index <- sample(1:nrow(uuu), 50000, replace = FALSE)
#uuu <- uuu[index, ]

# filter - create two dataframes, good vs bad residuals

uuu_good <- dplyr::tbl_df(uuu) %>%
  dplyr::filter(resid_context == '(-0.4,0.4]')

uuu_bad <- dplyr::tbl_df(uuu) %>%
  dplyr::filter(resid_context == '(-Inf,-0.4]')

uuu_good <- as.data.frame(uuu_good)
uuu_bad <- as.data.frame(uuu_bad)

# get the kde2d information:
# for good
uuu_good.kde <- kde2d(uuu_good[,2], uuu_good[,3], n = 400)
dx_good <- diff(uuu_good.kde$x[2:3])  # lifted from emdbook::HPDregionplot()
dy_good <- diff(uuu_good.kde$y[2:3])
sz_good <- sort(uuu_good.kde$z)
c1_good <- cumsum(sz_good) * dx_good * dy_good

# for bad
uuu_bad.kde <- kde2d(uuu_bad[,2], uuu_bad[,3], n = 400)
dx_bad <- diff(uuu_bad.kde$x[2:3])  # lifted from emdbook::HPDregionplot()
dy_bad <- diff(uuu_bad.kde$y[2:3])
sz_bad <- sort(uuu_bad.kde$z)
c1_bad <- cumsum(sz_bad) * dx_bad * dy_bad

# specify desired contour levels:
prob <- c(0.25, 0.5, 0.75, 0.90, 0.999)

# Configure plot
# for good
dimnames(uuu_good.kde$z) <- list(uuu_good.kde$x, uuu_good.kde$y)
dc_good <- melt(uuu_good.kde$z)
dc_good$prob <- approx(sz_good, 1 - c1_good, dc_good$value)$y

# for bad
dimnames(uuu_bad.kde$z) <- list(uuu_bad.kde$x, uuu_bad.kde$y)
dc_bad <- melt(uuu_bad.kde$z)
dc_bad$prob <- approx(sz_bad, 1 - c1_bad, dc_bad$value)$y

dc_good$residual <- '(-0.4,0.4]'
dc_bad$residual <- '(-Inf,-0.4]'
dc <- rbind(dc_good, dc_bad)

class_colors <- c('(-0.4,0.4]' = '#00BFC4', '(-Inf,-0.4]' = '#F8766D')

# Actual plot
p <- ggplot2::ggplot() +
  geom_contour(data = dc, aes(x = Var2, y = Var1, z = prob, color = residual), breaks = prob) +
  #geom_contour(data = dc_bad, aes(x = Var2, y = Var1, z = prob), breaks = prob, color = '#F8766D') + , color = '#00BFC4'
  labs(x = '(T_11 - T_12) * Reference SST', y = 'Temperature Deficit', color = 'Residual Range') +
  coord_fixed(ratio = 2.4) +
  xlim(-2, 122) +
  ylim(-.1, 22) + 
  scale_color_manual(values = class_colors) +
  ggsave('x2_vs_TD_cumulative_contours.svg', device = 'svg', width = 8, height = 6, units = 'in')

print(p)

```


```{r binning_retrievals_band_diff_temp_def_residual_a_la_raster, echo=TRUE}
# --- These numbers may help perform statistics for the matchups
# --- (e.g., number of matchups per cell, etc.).

temp_deficit_est <- orig$ref_SST - orig$T_11
temp_deficit <- orig$buoy.sst - orig$T_11

uuu <- data.frame(resid_context = as.factor(resid_context), temp_deficit_est = temp_deficit_est, product = (orig$band.diff * orig$ref_SST),
                  temp_deficit = temp_deficit,
                  x3 = orig$x3,
                  sec_satz = secant.deg(orig$satz),
                  SST.resid.SMB = orig$SST.resid.SMB)

# Take a sample of uuu bc loess requires lots of memory - for testing purposes
set.seed(42)
#index <- sample(1:nrow(uuu), 50000, replace = FALSE)
#uuu <- uuu[index, ]

# Horizontalize
td_estimate_fit <- predict(td_loess, newdata = uuu$product)
uuu$td_fit_error <- uuu$temp_deficit_est - td_estimate_fit

# --- Create a raster object with 5-degree pixels

grid5deg <- raster::raster(ncol = 20, nrow = 20,
  xmn = min(uuu$product) - 0.1, xmx = max(uuu$product) + 0.1,
  ymn = min(uuu$temp_deficit) - 0.1, ymx = max(uuu$temp_deficit) + 0.1)
  #crs = crs.string)

# --- Write cell numbers as value for the grids.
# --- Cell 1 is the upper left corner and numbers go
# --- across the top line through its end, then
# --- start again in the second line, and so forth.

raster::values(grid5deg) <- 1:raster::ncell(grid5deg) # Cell numbers for 5 deg grid

raster::cellFromRowCol(object = grid5deg, rownr = 2, colnr = 20)

raster::xyFromCell(object =  grid5deg, cell = 800, spatial = FALSE)

pts <- data.frame(product = uuu$product, temp_deficit = uuu$temp_deficit)
SST.resid.SMB <- uuu$SST.resid.SMB

count_good_residuals <- function(residuals, ...) {
  #if (length(residuals) < 82) {
  #  return(-Inf)
  #}
  resid_cats <- cut(residuals, c(-Inf, -0.4, 0.4))
  N_resid_bad_low <- table(resid_cats)[1]
  N_resid_good <- table(resid_cats)[2]
  fraction_good <- N_resid_good / (N_resid_bad_low + N_resid_good)
  return(fraction_good)
}

cell5deg <- raster::rasterize(pts, grid5deg, SST.resid.SMB, fun = count_good_residuals) # fun can be median, IQR, or count_good_residuals

#qqq <- dplyr::tbl_df(uuu) %>%
#  dplyr::group_by(cell5deg) %>%
#  dplyr::summarise(N = n(), med = median(SST.resid.SMB), IQR = IQR(SST.resid.SMB)) %>%
#  dplyr::filter(N >= 100)

#qq1 <- rep(NA, (20*40))
#qq1[qqq$cell5deg] <- qqq$med

#raster::values(grid5deg) <- qq1

blues_palette <- rev(RColorBrewer::brewer.pal(9, 'Blues'))
blues_palette <- colorRampPalette(blues_palette)(4)

reds_palette <- RColorBrewer::brewer.pal(9, 'Reds')
reds_palette <- colorRampPalette(reds_palette)(4)

palette <- c(blues_palette, '#f0f0f0', '#f0f0f0', reds_palette)
palette <- rev(palette)



#rasterVis::levelplot(cell5deg, margin = FALSE,
#  at = c(-Inf, seq(from = 0, to = 1.0, by = 0.05), Inf),
#  col.regions = palette,
#  xlab = '(T_11 - T_12) * Reference SST',
#  ylab = 'Temperature Deficit')

breaks <- seq(from = 0, to = 1, by = 0.1)

rasterVis::gplot(cell5deg) + 
  ggplot2::geom_raster(aes(fill = (cut(value, breaks)))) +
  scale_fill_manual(values = palette) +
  #scale_fill_gradientn(colours = palette) +
  labs(x = '(T_11 - T_12) * Reference SST', y = 'Temperature Deficit', fill = 'Fraction of\nGood Retrievals') +
  coord_fixed(ratio = 2.4) +
  xlim(-2, 122) +
  ylim(-.1, 22) + 
  ggsave('x2_vs_TD_raster.svg', device = 'svg', width = 8, height = 6, units = 'in')
```


As can be seen, there are very distinct regions of temperature deficit and product that have very high
probabilities of good (bad) residuals. However, there is also a notable region where there are roughly
equal portions of good and bad residuals. We build decision trees to segment those "in-between" retrievals.

# Decision Trees

## Prep Data for Decision Trees

### Create Additional Variables

Explored before is that lines can be drawn that are a function of the product and temperature deficit
that segment good and bad retrievals. However, we can't give the temperature deficit (the answer) to
the machine learning model. We create estimates of the temperature deficit using the reference SST
minus T_11 (T_12).

```{r create_additional_variables, echo=TRUE}
# Remove buoy_timedate
orig <- dplyr::tbl_df(orig) %>%
  dplyr::select(-buoy.timedate)

# Temperature Deficit Estimate
orig$ref_SST_minus_T_11 <- orig$ref_SST - orig$T_11

# Linearized temperature deficit estimate as a function of NLSST product
orig$td_resid <- orig$ref_SST_minus_T_11 - predict(td_loess, newdata = orig$x2)

# Loess can only interpolate
# Those values of x2 that are outside the domain get put in as NA
# Remove those
orig <- dplyr::tbl_df(orig) %>%
  dplyr::filter(!is.na(td_resid))

# Secant of the satz
orig$sec_satz <- secant.deg(orig$satz)

# Class of the Residual
orig$class_resid <- ifelse(orig$SST.resid.SMB <= -0.4, 'bad', 'good')
orig$class_resid <- factor(orig$class_resid, levels = c('bad', 'good'))

# Remove buoy_timedate
orig <- dplyr::tbl_df(orig) %>%
  dplyr::select(-buoy.timedate)
```

### Split into Train and Test Sets

All our algorithms will be trained on 60% of our data and tested on the other 40%.
We make that split here.

```{r create_train_test_set, echo=TRUE}
# Divide data into training and validation sets
prop_train <- .6
len_train <- floor(prop_train * nrow(orig))
index <- sample(1:nrow(orig), len_train, replace = FALSE)

orig_train <- orig[index, ]
orig_test <- orig[-index, ]
```

### Rebalance Classes
Most machine learning algorithms prefer to deal with balanced classes. As hinted at before, our classes
are heavily inbalanced, even after cutting down to just two classes (i.e., removing retrievals with
residuals greater than 0.4).

```{r inbalanced_clases, echo=TRUE}
table(cut(orig_train$SST.resid.SMB, c(-Inf, -0.4, 0.4)))

prop.table(table(cut(orig_train$SST.resid.SMB, c(-Inf, -0.4, 0.4))))
```

As can be seen from the above tables, the good retrievals are almost twice as prevalent as the bad
retrievals. Hence, we use rebalancing techniques, such as SMOTE and ROSE, to rebalance our dataset.
Based on a previous exploration, we saw that using the balanced dataset didn't always increase the
accuracy of the model when compared to not using the balanced data. However, using the balanced
data always resulted in higher sensitivity and specificity and at least comprable accuracy. Therefore,
we choose to stay with using the balanced dataset because the increased sensitivity and specificity
make us more likely to trust that model's predictions, regardless of which class they are for.


```{r rebalance classes, echo=TRUE}

orig_train_smote <- DMwR::SMOTE(class_resid ~ ., as.data.frame(orig_train), perc.over = 100, perc.under = 200)

# numInstances = # of majority class - # of minority class
# WHEN TESTING MWMOTE.... WE RAN OUT OF MEMORY
#numInstances <- table(orig_train$class_resid)['good'] - table(orig_train$class_resid)['bad']
#new_samples_mwmote <- imbalance::mwmote(as.data.frame(orig_train), numInstances = numInstances, classAttr = 'class_resid')

```

After rebalancing, our classes have almost equal prevalence.

```{r balanced_classes, echo=TRUE}
table(orig_train_smote$class_resid)

prop.table(table(orig_train_smote$class_resid))
```


## Trees

For all trees, we use the balanced dataset. We fit our model to the balanced train data and then
test on the test set. We create confusion matrices for both the train and test set, just to see
if the model is overfitting. We use three types of decision tree models: RPART, CTree, and Random
Forests. All trees receive the same 4 splitting variables: x2, sec(satz), temperature deficit estimate, 
and sd_11.

### RPART

```{r rpart_4_variable_tree, echo=TRUE}

# FASCINATING: No difference in model when we add latitude!

# Build the tree
rpart_4_var_tree <- rpart::rpart(formula = class_resid ~ td_resid + x2 + sec_satz + sd11,
                                 data = orig_train_smote,
                                 control = rpart::rpart.control(minbucket = 82, xval = 3))
# Plot the tree
plot(partykit::as.party(rpart_4_var_tree))

# Evaluate the tree
## Use the tree to predict
### Train set
rpart_4_var_tree_train_predictions <- stats::predict(object = rpart_4_var_tree, newdata = orig_train_smote, type = 'class', na.action = na.pass)

### Test set
rpart_4_var_tree_test_predictions <- stats::predict(object = rpart_4_var_tree, newdata = orig_test, type = 'class', na.action = na.pass)

## Build Confusion Matrices and print them
### Train set
rpart_4_var_tree_train_cm <- caret::confusionMatrix(data = rpart_4_var_tree_train_predictions, reference = orig_train_smote$class_resid)

rpart_4_var_tree_train_cm
### Test set
rpart_4_var_tree_test_cm <- caret::confusionMatrix(data = rpart_4_var_tree_test_predictions, reference = orig_test$class_resid)

rpart_4_var_tree_test_cm
```

```{r rpart_big_4_variable_tree, echo=TRUE}

# SAME THING: No effect of lat!

# Build the tree
rpart_big_4_var_tree <- rpart::rpart(formula = class_resid ~ td_resid + x2 + sec_satz + sd11,
                                 data = orig_train_smote,
                                 control = rpart::rpart.control(minbucket = 82, maxdepth = 7, xval = 3, cp = 0.0))
# Plot the tree
plot(partykit::as.party(rpart_big_4_var_tree))

# Evaluate the tree
## Use the tree to predict
### Train set
rpart_big_4_var_tree_train_predictions <- stats::predict(object = rpart_big_4_var_tree, newdata = orig_train_smote, type = 'class', na.action = na.pass)

### Test set
rpart_big_4_var_tree_test_predictions <- stats::predict(object = rpart_big_4_var_tree, newdata = orig_test, type = 'class', na.action = na.pass)

## Build Confusion Matrices and print them
### Train set
rpart_big_4_var_tree_train_cm <- caret::confusionMatrix(data = rpart_big_4_var_tree_train_predictions, reference = orig_train_smote$class_resid)

rpart_big_4_var_tree_train_cm
### Test set
rpart_big_4_var_tree_test_cm <- caret::confusionMatrix(data = rpart_big_4_var_tree_test_predictions, reference = orig_test$class_resid)

rpart_big_4_var_tree_test_cm
```


### CTree
```{r ctree_4_variable_tree, echo=TRUE}

# Again, no effect of lat

qqq <- orig_train_smote[, c('class_resid', 'td_resid', 'x2', 'sec_satz', 'sd11')]

names(qqq) <- c('class_resid', 'residual_temp_def_est', 'scaled_channel_diff', 'sec_satz', 'SD_11')

# Build the tree
ctree_4_var_tree <- partykit::ctree(formula = class_resid ~ .,
                                    data = qqq,
                                    minbucket = 82,
                                    mincrit = 0.95,
                                    maxdepth = 4)
# Plot the tree
plot(ctree_4_var_tree, type = 'simple')

# Evaluate the tree
## Use the tree to predict
### Train set
#ctree_4_var_tree_train_predictions <- stats::predict(object = ctree_4_var_tree, newdata = orig_train_smote, type = 'response', na.action = na.pass)

### Test set
#ctree_4_var_tree_test_predictions <- stats::predict(object = ctree_4_var_tree, newdata = orig_test, type = 'response', na.action = na.pass)

## Build Confusion Matrices and print them
### Train set
#ctree_4_var_tree_train_cm <- caret::confusionMatrix(data = ctree_4_var_tree_train_predictions, reference = orig_train_smote$class_resid)

#ctree_4_var_tree_train_cm
### Test set
#ctree_4_var_tree_test_cm <- caret::confusionMatrix(data = ctree_4_var_tree_test_predictions, reference = orig_test$class_resid)

#ctree_4_var_tree_test_cm
```


```{r ctree_4_variable_tree_big, echo=TRUE}

# again, no change with lat

# Build the tree
ctree_4_var_tree_big <- partykit::ctree(formula = class_resid ~ td_resid + x2 + sec_satz + sd11,
                                    data = orig_train_smote,
                                    minbucket = 82,
                                    mincrit = 0.95,
                                    maxdepth = 7)
# Plot the tree
#plot(ctree_4_var_tree_big)

# Evaluate the tree
## Use the tree to predict
### Train set
ctree_4_var_tree_big_train_predictions <- stats::predict(object = ctree_4_var_tree_big, newdata = orig_train_smote, type = 'response', na.action = na.pass)

### Test set
ctree_4_var_tree_big_test_predictions <- stats::predict(object = ctree_4_var_tree_big, newdata = orig_test, type = 'response', na.action = na.pass)

## Build Confusion Matrices and print them
### Train set
ctree_4_var_tree_big_train_cm <- caret::confusionMatrix(data = ctree_4_var_tree_big_train_predictions, reference = orig_train_smote$class_resid)

ctree_4_var_tree_big_train_cm
### Test set
ctree_4_var_tree_big_test_cm <- caret::confusionMatrix(data = ctree_4_var_tree_big_test_predictions, reference = orig_test$class_resid)

ctree_4_var_tree_big_test_cm
```


```{r ctree_variable_importance, echo=TRUE}

variable_names_long <- c('Except\nResidual Temperature Deficit Estimate',
                        'Except\nScaled Channel Difference',
                        'Except\nsecant(satellite zenith angle)',
                        'Except\nSD of T_11')

variable_names_abbrev <- c('td_resid',
                           'x2',
                           'sec_satz',
                           'sd11')

empty <- c(0, 0, 0, 0)
accuracy_without <- data.frame(variable_names_long = variable_names_long, variable_names_abbrev = variable_names_abbrev, accuracy = empty)

for (variable_num in 1:nrow(accuracy_without)) {
  iii <- c(1, 2, 3, 4)
  iii <- iii[!iii %in% variable_num]
  variables_temp <- variable_names_abbrev[iii]
  ppp <- orig_train_smote[, c(variables_temp, 'class_resid')]
  
  ctree_accuracy_test <- partykit::ctree(formula = class_resid ~ .,
                                    data = ppp,
                                    minbucket = 82,
                                    mincrit = 0.95,
                                    maxdepth = 7)
  
  ctree_accuracy_test_predictions <- stats::predict(object = ctree_accuracy_test, newdata = orig_test, type = 'response', na.action = na.pass)

  ctree_accuracy_test_cm <- caret::confusionMatrix(data = ctree_accuracy_test_predictions, reference = orig_test$class_resid)

  # Now compute balanced accuracy
  sensitivity <- ctree_accuracy_test_cm$table[1] / (ctree_accuracy_test_cm$table[1] + ctree_accuracy_test_cm$table[3])
  specificity <- ctree_accuracy_test_cm$table[4] / (ctree_accuracy_test_cm$table[2] + ctree_accuracy_test_cm$table[4])
  
  balanced_accuracy <- (sensitivity + specificity) / 2
  
  accuracy_without[variable_num, 'accuracy'] <- balanced_accuracy

}

# 0.7812 is the balanced accuracy on the test set with ALL variables (from above)
accuracy_without$accuracy_decr <- 0.7812 - accuracy_without$accuracy

accuracy_without$variable_names_long <- reorder(accuracy_without$variable_names_long, -accuracy_without$accuracy)

ggplot2::ggplot(data = accuracy_without, aes(x = variable_names_long, y = accuracy_decr)) +
  geom_bar(stat = 'identity') +
  theme(axis.text.x = element_text(angle = 45, hjust = 0.5, vjust = 0.5)) +
  labs(x = 'Feature', y = 'Decrease in Balanced Accuracy') +
  ggsave('variable_importance_accuracy_ctree.pdf', device = 'pdf', width = 8, height = 6, units = 'in')
```


```{r analysis_of_disagreements, echo=TRUE}
if_disagree <- ctree_4_var_tree_big_test_predictions != orig_test$class_resid

disagreements <- orig_test[if_disagree, ]


summary(disagreements$SST.resid.SMB)
sd(disagreements$SST.resid.SMB)

summary(orig_test$SST.resid.SMB)
sd(orig_test$SST.resid.SMB)

t.test(disagreements$SST.resid.SMB, orig_test$SST.resid.SMB)


summary(disagreements$SST.resid.SMB)
sd(disagreements$SST.resid.SMB)

summary(orig_train_smote$SST.resid.SMB)
sd(orig_train_smote$SST.resid.SMB)

t.test(disagreements$SST.resid.SMB, orig_train_smote$SST.resid.SMB)
```

### Random Forest

```{r random_forest_4_variable_tree, echo=TRUE}

# Lat has negligable effect - mean decrease gini very close to 0, change of accuracy is within CI

# Build the tree
rand_for_4_var <- randomForest::randomForest(formula = class_resid ~ td_resid + x2 + sec_satz + sd11,
                                             data = orig_train_smote,
                                             nodesize = 82,
                                             ntree = 750,
                                             mtry = 2)
# Plot the tree
randomForest::varImpPlot(rand_for_4_var)

# Evaluate the tree
## Use the tree to predict
### Train set
rand_for_4_var_train_predictions <- stats::predict(object = rand_for_4_var, newdata = orig_train_smote, type = 'class', na.action = na.pass)

### Test set
rand_for_4_var_test_predictions <- stats::predict(object = rand_for_4_var, newdata = orig_test, type = 'class', na.action = na.pass)

## Build Confusion Matrices and print them
### Train set
rand_for_4_var_train_cm <- caret::confusionMatrix(data = rand_for_4_var_train_predictions, reference = orig_train_smote$class_resid)

rand_for_4_var_train_cm
### Test set
rand_for_4_var_test_cm <- caret::confusionMatrix(data = rand_for_4_var_test_predictions, reference = orig_test$class_resid)

rand_for_4_var_test_cm
```

### Regression

```{r ctree_big_4_variable_tree_regr, echo=TRUE}

# Build the tree
ctree_big_4_var_tree_regr <- partykit::ctree(formula = SST.resid.SMB ~ td_resid + x2 + sec_satz + sd11,
                                 data = orig_train,
                                 minbucket = 82,
                                 maxdepth = 7,
                                 mincrit = 0.95)
# Plot the tree
#plot(partykit::as.party(rpart_big_4_var_tree_regr))

# Evaluate the tree
## Use the tree to predict
### Train set
ctree_big_4_var_tree_train_vals <- stats::predict(object = ctree_big_4_var_tree_regr, newdata = orig_train, na.action = na.pass)

### Test set
ctree_big_4_var_tree_test_vals <- stats::predict(object = ctree_big_4_var_tree_regr, newdata = orig_test, na.action = na.pass)

### Train set
rmse_train_ctree <- caret::RMSE(ctree_big_4_var_tree_train_vals, orig_train$SST.resid.SMB)
rmse_train_ctree
cor(ctree_big_4_var_tree_train_vals, orig_train$SST.resid.SMB)

### Test set
rmse_test_ctree <- caret::RMSE(ctree_big_4_var_tree_test_vals, orig_test$SST.resid.SMB)
rmse_test_ctree
cor(ctree_big_4_var_tree_test_vals, orig_test$SST.resid.SMB)

ggplot2::ggplot() +
  geom_point(aes(x = ((ctree_big_4_var_tree_test_vals)), orig_test$SST.resid.SMB), pch = '.')
```

```{r evaluate_ctree_regression_statistically, echo=TRUE}

window <- .1

dt_model_residuals_train <- ctree_big_4_var_tree_train_vals - orig_train$SST.resid.SMB

table(abs(dt_model_residuals_train) < window)
prop.table(table(abs(dt_model_residuals_train) < window))
IQR(dt_model_residuals_train)


dt_model_residuals_test <- ctree_big_4_var_tree_test_vals - orig_test$SST.resid.SMB

table(abs(dt_model_residuals_test) < window)
prop.table(table(abs(dt_model_residuals_test) < window))
IQR(dt_model_residuals_test)


```


```{r explore_regression_ctree}
ctree_big_4_var_tree_train_nodes <- stats::predict(object = ctree_big_4_var_tree_regr, newdata = orig_train, type = 'node', na.action = na.pass)

medians_ctree <- tapply(orig_train$SST.resid.SMB, ctree_big_4_var_tree_train_nodes, median)
IQRs_ctree <- tapply(orig_train$SST.resid.SMB, ctree_big_4_var_tree_train_nodes, IQR)
lengths_ctree <- tapply(orig_train$SST.resid.SMB, ctree_big_4_var_tree_train_nodes, length)

node_predictions_ctree <- data.frame(SST.resid.SMB = orig_train$SST.resid.SMB, node = ctree_big_4_var_tree_train_nodes)

ggplot2::ggplot(data = node_predictions_ctree) +
  geom_histogram(aes(SST.resid.SMB)) +
  facet_wrap(~node)

```


## Cubist

```{r M5P, echo=TRUE}
orig_train <- dplyr::tbl_df(orig_train) %>%
  dplyr::filter(!is.na(td_resid))

orig_train <- as.data.frame(orig_train)

M5P_tree <- Cubist::cubist(x = orig_train[, c('td_resid', 'x2', 'sec_satz', 'sd11')], y = orig_train$SST.resid.SMB,
                           committees = 1,
                           control = Cubist::cubistControl(rules = 46))

capture.output(summary(M5P_tree), file = 'M5P_tree_rules_and_stats_rules=46.txt')

dotplot(M5P_tree, what = 'splits')
dotplot(M5P_tree, what = 'coefs')

# Evaluate the tree
## Use the tree to predict
### Train set
N_correction_neighbors <- 0 # Must be between 0 and 9

M5P_tree_train_vals <- stats::predict(object = M5P_tree, newdata = orig_train, na.action = na.pass, neighbors = N_correction_neighbors)

### Test set
M5P_tree_test_vals <- stats::predict(object = M5P_tree, newdata = orig_test, na.action = na.pass, neighbors = N_correction_neighbors)

### Train set
rmse_train_M5P <- caret::RMSE(M5P_tree_train_vals, orig_train$SST.resid.SMB)
rmse_train_M5P
cor(M5P_tree_train_vals, orig_train$SST.resid.SMB)

### Test set
rmse_test_M5P <- caret::RMSE(M5P_tree_test_vals, orig_test$SST.resid.SMB)
rmse_test_M5P
cor(M5P_tree_test_vals, orig_test$SST.resid.SMB)

ggplot2::ggplot() +
  geom_point(aes(x = M5P_tree_test_vals, orig_test$SST.resid.SMB), pch = '.') +
  geom_abline(slope = 1, intercept = 0, color = 'tomato') +
  labs(x = 'Predicted Residual from the Cubist Decision Tree Regression Algorithm', y = 'Actual Residual')
  #ggsave('M5P_predictions_vs_actual_residual.svg', device = 'svg', width = 8, height = 6, units = 'in')



ccc <- RColorBrewer::brewer.pal(n = 9, 'YlOrRd')
ccc <- ccc[3:9]
breaks <- c(0, 10, 25, 50, 100, 500, 1000, 2500, 5000, 10000, Inf)

yyy <- data.frame(Y = M5P_tree_test_vals, X = orig_test$SST.resid.SMB)

ccc <- colorRampPalette(ccc)(length(breaks) - 1)

ggplot2::ggplot(data = yyy, aes(X, Y, fill = cut(..count.., c(0, 10, 25, 50, 100, 500, 1000, 2500, 5000, 10000, Inf)))) +
  geom_bin2d(bins = 75) +
  scale_fill_manual(values = ccc, '# of Retrievals') +
  geom_abline(slope = 1, intercept = 0, color = 'black') +
  labs(x = 'Actual Residual', y = 'Predicted Residual from the Cubist Decision Tree Regression Algorithm') +
  ggsave('Cubist_predictions_vs_actual_residual_2d_histogram.pdf', device = 'pdf', width = 8, height = 6, units = 'in')

```

```{r evaluate_M5P_regression_statistically, echo=TRUE}

window <- .2

M5P_model_residuals_train <- M5P_tree_train_vals - orig_train$SST.resid.SMB

table(abs(M5P_model_residuals_train) < window)
prop.table(table(abs(M5P_model_residuals_train) < window))
IQR(M5P_model_residuals_train)


M5P_model_residuals_test <- M5P_tree_test_vals - orig_test$SST.resid.SMB

table(abs(M5P_model_residuals_test) < window)
prop.table(table(abs(M5P_model_residuals_test) < window))
IQR(M5P_model_residuals_test)

```

```{r evaluate_M5P_regression_statistically_plot, echo=TRUE}
windows <- seq(from = 0, to = 1.0, by = .05)

prob_in_window <- windows

for (i in seq(1, length(windows))) {
  M5P_model_residuals_test <- M5P_tree_test_vals - orig_test$SST.resid.SMB

  window <- windows[i]
  ooo <- prop.table(table(abs(M5P_model_residuals_test) < window))
  prob_in_window[i] <- ooo[2]
}

prob_in_window <- prob_in_window * 100

ggplot2::ggplot() +
  geom_point(aes(x = windows, y = prob_in_window)) +
  xlim(0, 1.0) +
  ylim(0, 100) +
  labs(x = 'Plus/Minus Interval', y = '% Retrievals with abs(Predicted - Actual Residual) < Interval') +
  ggsave('M5P_predictions_within_interval.svg', device = 'svg', width = 8, height = 6, units = 'in')

```


```{r rule_distribution_ctree, echo=TRUE}
# Rule 1:
rule1 <- dplyr::tbl_df(orig_train) %>%
  dplyr::filter(td_resid > 1.013252) %>%
  dplyr::filter(x2 <= 21.7106) %>%
  dplyr::filter(sec_satz <= 1.536021) %>%
  dplyr::filter(sd11 <= 0.1656)

ggplot2::ggplot() +
  geom_histogram(data = rule1, aes(SST.resid.SMB), bins = 20) +
  xlim(-7, 0.5) +
  labs(x = 'SST Residual', y = 'N of Retrievals') +
  ggsave('Cubist_residual_distribution_rule_1.pdf', device = 'pdf', width = 8, height = 6, units = 'in')



rule2 <- dplyr::tbl_df(orig_train) %>%
  dplyr::filter(td_resid > 0.4383563) %>%
  dplyr::filter(x2 > 21.7106) %>%
  dplyr::filter(x2 <= 53.802) %>%
  dplyr::filter(sec_satz <= 1.536021) %>%
  dplyr::filter(sd11 > 0.1108)

ggplot2::ggplot() +
  geom_histogram(data = rule2, aes(SST.resid.SMB), bins = 20) +
  xlim(-7, 0.5) +
  labs(x = 'SST Residual', y = 'N of Retrievals') +
  ggsave('Cubist_residual_distribution_rule_2.pdf', device = 'pdf', width = 8, height = 6, units = 'in')

```


