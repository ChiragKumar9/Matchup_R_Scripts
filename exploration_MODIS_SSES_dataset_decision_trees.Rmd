---
title: "Exploration of a MODIS SSES Dataset - Decision Trees"
author: "Chirag Kumar and Guillermo Podesta"
output:
  html_notebook:
    toc: True
    theme: united
---

# Prep Workspace and Data
We use MODIS matchups from 2002 - 2016 that have been read in through a previous script. Here we simply load
the matchups, any necessary packages, and define necessary functions.

```{r prep_workspace, include=FALSE}
# Import necessary packages
require(ggplot2)
require(dplyr)
require(RColorBrewer)
require(circular)
require(ggmap)
require(raster)
require(sfsmisc)
require(rasterVis)
require(rgdal)
require(rpart)
require(rpart.plot)
require(caret)
require(evtree)
require(ctree)
require(partykit)
require(randomForest)

ggplot <- function(...) {ggplot2::ggplot(...) + theme_bw()}

# Define secant function
secant.deg <- function(x) {1 / (cos(circular::rad(x)))}

# Define function to turn output of gettree from random forest into a dendrogram
to.dendrogram <- function(dfrep,rownum=1,height.increment=0.1){

  if(dfrep[rownum,'status'] == -1){
    rval <- list()

    attr(rval,"members") <- 1
    attr(rval,"height") <- 0.0
    attr(rval,"label") <- dfrep[rownum,'prediction']
    attr(rval,"leaf") <- TRUE

  }else{##note the change "to.dendrogram" and not "to.dendogram"
    left <- to.dendrogram(dfrep,dfrep[rownum,'left daughter'],height.increment)
    right <- to.dendrogram(dfrep,dfrep[rownum,'right daughter'],height.increment)
    rval <- list(left,right)

    attr(rval,"members") <- attr(left,"members") + attr(right,"members")
    attr(rval,"height") <- max(attr(left,"height"),attr(right,"height")) + height.increment
    attr(rval,"leaf") <- FALSE
    attr(rval,"edgetext") <- dfrep[rownum,'split var']
    #To add Split Point in Dendrogram
    #attr(rval,"edgetext") <- paste(dfrep[rownum,'split var'],"\n<",round(dfrep[rownum,'split point'], digits = 2),"=>", sep = " ")
  }

  class(rval) <- "dendrogram"

  return(rval)
}

# Source own functions
# Define direwctory where functions are for each operating system

if (Sys.info()["sysname"] == 'Windows') {
  fun.dir <- 'D:/matchups/r-projects/Matchup_R_Scripts/Functions/'
} else if (Sys.info()["sysname"] == 'Linux') {
  fun.dir <- '/home/ckk/Projects/Matchup_R_Scripts/Functions/'
}

fun.file <- paste0(fun.dir, 'common_functions.R')

source(file = fun.file,
  local = FALSE, echo = FALSE, verbose = FALSE)
rm(fun.dir, fun.file)


# Load data
# For 787
linux_dir <- '~/Projects/Matchup_R_Scripts/Results/objects/'
# For Laptop
#linux_dir <- '~/Projects/Matchup_R_Scripts/'
linux_file <- 'MODIS_Aqua_GSFC_ALL_Class_6.4.1_ao_2017_04_12_with_ancillary.Rdata'
AQUA_file <- paste0(linux_dir, linux_file)

if (!file.exists(AQUA_file)) {
    stop('Input file does not exist')
  } else {
    load(AQUA_file, verbose = TRUE)
    AQUA <- MODIS_Aqua_GSFC_ALL_Class_6.4.1_ao_2017_04_12
  }

rm(linux_dir, linux_file, AQUA_file)
rm(MODIS_Aqua_GSFC_ALL_Class_6.4.1_ao_2017_04_12)

set.seed(42)
```

The matchup file read in at a previous step is not filtered and contains too many variables. For the purpose of 
this analysis, we use only nighttime matchups with a quality of 0, 1, or 2 and only keep variables from the infrared
channels we use.

```{r prep_data, echo=TRUE}
# Turn POSIXct objects to characters bc dplyr doesn't support POSIXct
AQUA$sat.timedate <- as.character(AQUA$sat.timedate)
AQUA$buoy.timedate <- as.character(AQUA$buoy.timedate)

# Apply basic filtering
AQUA <- dplyr::tbl_df(AQUA) %>%
  dplyr::filter(solz >= 90) %>%
  dplyr::filter(qsst == 0 | qsst == 1 | qsst == 2)

# Now grab only variables that may be used to determine retrieval accuracy and make some new variables (i.e. x1, x2, x3)
# Create df of features
orig <- dplyr::tbl_df(AQUA) %>%
  dplyr::mutate(T_11 = cen.11000,
    T_12 = cen.12000,
    band.diff = T_11 - T_12,
    ref_SST = cen.ref.type.1.SST,
    x2 = band.diff * ref_SST,
    x3 = ((secant.deg(satz) - 1) * band.diff),
    lat = buoy.lat,
    lon = buoy.lon,
    sd11 = sd.11000,
    sd12 = sd.12000,
    range11 = max.11000 - min.11000,
    range12 = max.12000 - min.12000,
    diff.med.min11 = med.11000 - min.11000,
    diff.med.min12 = med.12000 - min.12000,
    qsst = qsst,
    buoy.sst = buoy.sst,
    cell5deg = cell5deg,
    buoy.timedate = buoy.timedate,
    SST.resid.SMB = cen.sst - buoy.sst) %>% # SMB = sat minus buoy - also don't debias SSTs i.e. turn buoy into skin
  dplyr::select(T_11, T_12, band.diff, ref_SST, x2, x3, satz, lon, lat, sd11, sd12, range11, range12, diff.med.min11, diff.med.min12,
    qsst, buoy.sst, cell5deg, buoy.timedate, SST.resid.SMB)

```

# Decision Trees

Here we explore the ability of decision trees to accurately classify SST matchups that fall in defined residual
ranges.  First we prep the data into training and validation sets for the decision tree exploration.

## Prep Data for Decision Trees

```{r prep_data_decision_trees, echo=TRUE}

# Debias SST resid - i.e. account for skin to bulk conversion
orig$SST.resid.SMB <- orig$SST.resid.SMB + 0.17

# Make classes
class_resid <- cut(orig$SST.resid.SMB, breaks = c((min(orig$SST.resid.SMB)),
                                                  -.4,
                                                  .4,
                                                  (max(orig$SST.resid.SMB + 0.01))),
                   labels = c('bad_low', 'good', 'bad_high'), include.lowest = TRUE)
#class_resid <- ifelse(orig$SST.resid.SMB > .2, 'resid > .2', ifelse(orig$SST.resid.SMB < -.2, 'resid < -.2', '-.2 <= resid <= .2'))
orig$class_resid <- as.factor(class_resid)
table(orig$class_resid)

orig %>% dplyr::group_by(class_resid) %>%
  dplyr::summarise(min = min(SST.resid.SMB), max = max(SST.resid.SMB))

# Divide data into training and validation sets
prop_train <- .6
len_train <- floor(prop_train * nrow(orig))
index <- sample(1:nrow(orig), len_train, replace = FALSE)

orig_train <- orig[index, ]
orig_test <- orig[-index, ]

```


## rpart Decision Trees

```{r rpart_decision_tree, echo=TRUE}

# Train a stard rpart decision tree on above data
rpart_tree <- rpart::rpart(formula = class_resid ~ T_11 + T_12 + band.diff + ref_SST + (ref_SST - T_11) + (ref_SST - T_12) + x2 +
    abs(satz) + range11 + sd11 + range12 + sd12 + x3, 
                           data = orig_train,
                           minbucket = 100,
                           xval = 5)

# Plot tree
rpart.plot::prp(rpart_tree, uniform = TRUE, faclen = 0, varlen = 0, extra = 4,
  main = "rpart Classification Tree for Identifying Matchups' Residual Range")

# Plot the tree as a partykit object
plot(partykit::as.party(rpart_tree))

# Evaluate the decision tree's accuracy on validation set
rpart_tree_predictions <- predict(object = rpart_tree, newdata = orig_test, type = 'class', na.action = na.pass)

# Draw confusion matrix for predictions on rpart tree
rpart_tree_cm <- caret::confusionMatrix(data = rpart_tree_predictions, reference = orig_test$class_resid)
rpart_tree_cm

```

## ctree Decision Trees

```{r ctree, echo=TRUE}
ctree_tree <- partykit::ctree(formula = class_resid ~ T_11 + T_12 + band.diff + ref_SST + (ref_SST - T_11) + (ref_SST - T_12) + x2 +
    abs(satz) + range11 + sd11 + range12 + sd12 + x3,
                              data = orig_train,
                              minbucket = 100,
                              mincrit = .95,
                              maxdepth = 5)

# Plot the tree
plot(ctree_tree)

# Evaluate the decision tree's accuracy on validation set
ctree_tree_predictions <- predict(ctree_tree, newdata = orig_test, type = 'response', na.action = na.pass)

# Draw confusion matrix for predictions on rpart tree
ctree_tree_cm <- caret::confusionMatrix(data = ctree_tree_predictions, reference = orig_test$class_resid)
ctree_tree_cm
```


## cforest Decision Forests
```{r cforest, echo=TRUE}

cforest_clf <- partykit::cforest(formula = class_resid ~ T_11 + T_12 + band.diff + ref_SST + (ref_SST - T_11) + (ref_SST - T_12) + x2 +
    abs(satz) + range11 + sd11 + range12 + sd12 + x3,
                              data = orig_train,
                              minbucket = 100,
                              control = ctree_control(mincriterion = .95),
                              maxdepth = 5,
                              ntree = 500)

# Trying to visualize the forest...
plot(cforest_clf)

# Evaluate the decision tree's accuracy on validation set
cforest_clf_predictions <- predict(ctree_tree, newdata = orig_test, type = 'response', na.action = na.pass)

# Draw confusion matrix for predictions on rpart tree
cforest_clf_cm <- caret::confusionMatrix(data = ctree_tree_predictions, reference = orig_test$class_resid)
cforest_clf_cm
```


## Random Forest Decision Forests

```{r random_forest, echo=TRUE}
orig_train$abs_satz <- abs(orig_train$satz)
orig_test$abs_satz <- abs(orig_test$satz)
orig_test$ref_SST_minus_T_11 <- orig_test$ref_SST - orig_test$T_11
orig_test$ref_SST_minus_T_12  <- orig_test$ref_SST - orig_test$T_12
rand_for <- randomForest::randomForest(formula = class_resid ~ T_11 + T_12 + band.diff + ref_SST + (ref_SST - T_11) + (ref_SST - T_12) + x2 +
    abs_satz + range11 + sd11 + range12 + sd12 + x3,
                                       data = orig_train,
                                       ntree = 500, # how many trees should be in the forest we grow
                                       nodesize = 500, # same as minbucket in rpart
                                       mtry = 3) # how many variables the rf randomly picks at each branch to do splitting

# Trying to visualize the tree...
plot(rand_for)
randomForest::varImpPlot(rand_for)
#randomForest::MDSplot(rand_for, orig_train$class_resid)
#randomForest::partialPlot(rand_for, orig_train, x.var = 'band.diff')
#randomForest::plot.randomForest(rand_for)

# Plot a specific tree
tree_example <- randomForest::getTree(rand_for, k = 1, labelVar = TRUE)
tree_example_dendro <- to.dendrogram(tree_example)
str(tree_example_dendro)
plot(tree_example_dendro, center = TRUE, leaflab = 'none', edgePar = list(t.cex = 1, p.col = NA, p.lty = 0))

# Evaluate the decision tree's accuracy on validation set
rand_for_predictions <- predict(rand_for, newdata = orig_test, type = 'class', na.action = na.pass)

# Draw confusion matrix for predictions on evtree tree
rand_for_cm <- caret::confusionMatrix(data = rand_for_predictions, reference = orig_test$class_resid)
rand_for_cm
```


## Evolutionary Decision Trees

```{r evtree_decision_tree_v1, echo=TRUE}

# Further sampling for the evtree because it takes a long time...
index_evtree <- sample(1:nrow(orig_train), 100, replace = FALSE)
orig_train_subset <- orig_train[index_evtree, ]

# Train a stard evtree decision tree on above data
evtree_tree <- evtree::evtree(formula = class_resid ~ T_11 + sd11, #T_11 + band.diff + ref_SST + satz + range11 + sd11,
                              data = orig_train, 
                              minsplit = 10)

# Plot tree
plot(evtree_tree)

# Evaluate the decision tree's accuracy on validation set
evtree_tree_predictions <- predict(evtree_tree, newdata = orig_test, type = 'class', na.action = na.pass)

# Draw confusion matrix for predictions on evtree tree
evtree_tree_cm <- caret::confusionMatrix(data = evtree_tree_predictions, reference = orig_test$class_resid)
evtree_tree_cm

```

Second iteration - further testing...
```{r evtree_decision_tree_v2, echo=TRUE}

# Further sampling for the evtree because it takes a long time...
index_evtree <- sample(1:nrow(orig_train), 10000, replace = FALSE)
orig_train_subset <- orig_train[index_evtree, ]

# Train a stard evtree decision tree on above data
evtree_tree <- evtree::evtree(formula = class_resid ~ T_11 + T_12 + band.diff + ref_SST + (ref_SST - T_11) + (ref_SST - T_12) + x2 +
    abs(satz) + range11 + sd11 + range12 + sd12 + x3, 
                           data = orig_train, 
                           minbucket = 100)

# Plot tree
plot(evtree_tree)

# Evaluate the decision tree's accuracy on validation set
evtree_tree_predictions <- predict(evtree_tree, newdata = orig_test, type = 'class', na.action = na.pass)

# Draw confusion matrix for predictions on evtree tree
evtree_tree_cm <- caret::confusionMatrix(data = evtree_tree_predictions, reference = orig_test$class_resid)
evtree_tree_cm

```


# Testing...

## Testing evtree
```{r testing_evtree, echo=TRUE}
# Example straight from evtree vignette
# Load data
data('BBBClub', package = 'evtree')
head(BBBClub)
dim(BBBClub)

# Now fit tree
set.seed(1090)
ev <- evtree(choice ~ ., data = BBBClub, minbucket = 10, maxdepth = 2)
plot(ev)
ev

```

## Testing cforest
```{r testing_cforest, echo=TRUE}
library (party)

data ("GlaucomaMVF", package = "ipred")

inputData <- GlaucomaMVF  # plugin your data here, with 'Class' as dependent variable

set.seed (100)

train <- sample(1:nrow(inputData), 0.7*nrow(inputData))  # random sample

trainData <- inputData[train,]  # training data

testData <- inputData[-train,]  # test data

cTreeMod <- ctree (Class ~ ., data = trainData)  # fit cTree with 'Class' as dependent

actuals <- testData$Class # actuals

predicted <- predict(cTreeMod, newdata = testData) # predicted

table(true = actuals, pred = predicted) # confusion matrix

mean (testData$Class != predicted) # Misclassification Error %

cForestMod <- cforest(Class ~ ., data = trainData)  # random Forest model

actuals <- testData$Class # actuals

predicted <- predict(cTreeMod, newdata = testData) # predicted

table (true = actuals, pred = predict(cForestMod, newdata = testData))

mean (testData$Class != predicted) # Misclassification Error %
```




